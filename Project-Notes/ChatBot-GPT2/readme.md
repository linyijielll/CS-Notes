# ChatBot-GPT2ç¬”è®°

è®°å½•ä¸€ä¸‹é¡¹ç›®å¼€å‘ä¸­çš„ç†è§£ï¼Œæ”¶è·ï¼Œæ€è€ƒğŸ¤”ã€‚[é¡¹ç›®ä¼ é€é—¨](https://github.com/chenjunyi1999/ChatBot-Pytorch)

è¿™é‡Œå°½é‡æŒ‰ç…§é¡¹ç›®æ–‡ä»¶ç»“æ„é€ä¸€ç¼–å†™ã€‚



# GPT2æ¨¡å‹

è¯·åœ¨[chenjunyi1999/ML-Tutorial/æ¨¡å‹ç¬”è®°](https://github.com/chenjunyi1999/ML-Tutorial/tree/main/%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0)ä¸­æŸ¥çœ‹ç¬”è®°

ä¹Ÿå¯ä»¥åœ¨[huggingface](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.TFGPT2LMHeadModel)å®˜ç½‘æŸ¥çœ‹æ¨¡å‹ç»†èŠ‚



# main.py

`main.py` æ˜¯é¡¹ç›®çš„æ¨¡å‹ä¸»æ–‡ä»¶ï¼Œè·å–å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆå§‹åŒ–`tokenizer` ï¼Œ`model`ï¼Œ `optimizer` ç­‰ï¼Œå¹¶æ ¹æ®å‚æ•°å®Œæˆæ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹æ¨ç†

## argparse

**argsparse**æ˜¯pythonçš„å‘½ä»¤è¡Œè§£æçš„æ ‡å‡†æ¨¡å—ï¼Œå†…ç½®äºpythonã€‚ä½¿ç”¨è¿™ä¸ªæ¨¡å—å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨å‘½ä»¤è¡Œä¸‹ä¼ å…¥å‚æ•°è€Œä¸éœ€è¦åœ¨ä»£ç çš„config ä¸­ä¿®æ”¹ã€‚

```python
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--mode', default='train', type=str, required=False, help="The running mode: train or inference?")
parser.add_argument('--ckpt_name', type=str, required=False, help="The name of the trained checkpoint. (without extension)")
args = parser.parse_args()
```

å¯ä»¥ä½¿ç”¨ `python demo.py -h` å‘½ä»¤æ¥æŸ¥çœ‹ä¼ å…¥å‚æ•°çš„è¯´æ˜

![argparse_help](./img/argparse_help.png)

å¾—åˆ°çš„ç»“æœç±»ä¼¼äºpythonå­—å…¸ï¼Œå¯ä»¥ç”¨`args.å‚æ•°å` æ¥æå–è¿™ä¸ªå‚æ•°

### ä¸‹é¢æ˜¯ä¸€äº›å¸¸ç”¨å‚æ•°çš„ä½¿ç”¨æ–¹æ³•

- `nargs='+'` : nargsæ˜¯ç”¨æ¥è¯´æ˜ä¼ å…¥çš„å‚æ•°ä¸ªæ•°ï¼Œ'+' è¡¨ç¤ºä¼ å…¥è‡³å°‘ä¸€ä¸ªå‚æ•°
  `nargs='*' ` : è¡¨ç¤ºå‚æ•°å¯è®¾ç½®é›¶ä¸ªæˆ–å¤šä¸ª
  `nargs='?'` : è¡¨ç¤ºå‚æ•°å¯è®¾ç½®é›¶ä¸ªæˆ–ä¸€ä¸ª

  ```python
  parser = argparse.ArgumentParser(description='å‘½ä»¤è¡Œä¸­ä¼ å…¥ä¸€ä¸ªæ•°å­—')
  parser.add_argument('integers', type=str,help='ä¼ å…¥çš„æ•°å­—')
  args = parser.parse_args()
  print(args.integers)
  >>> python main.py 1 2 3 4
  <<< error: unrecognized arguments: 2 3 4
      
  parser = argparse.ArgumentParser(description='å‘½ä»¤è¡Œä¸­ä¼ å…¥ä¸€ä¸ªæ•°å­—')
  parser.add_argument('integers', type=str,nargs='+',help='ä¼ å…¥çš„æ•°å­—')
  args = parser.parse_args()
  print(args.integers)
  >>> python main.py 1 2 3 4
  <<< ['1', '2', '3', '4']
  ```

- `type` : å¯ä»¥è®¾ç½®ä¼ å…¥å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œå¯ä»¥ä¼ å…¥str,int,list, str, tuple, set, dictç­‰

- `å¯é€‰å‚æ•°--` ï¼šä¸ºäº†åœ¨å‘½ä»¤è¡Œä¸­é¿å…ä¸Šè¿°ä½ç½®å‚æ•°ä¼ å…¥é”™è¯¯ï¼ˆå¿˜äº†é¡ºåºå‡ºé”™ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨å¯é€‰å‚æ•°ï¼Œéœ€è¦åœ¨å…³é”®è¯å‰é¢åŠ `--`

  ```python
  parser = argparse.ArgumentParser()
  parser.add_argument('--mode', default='train', type=str, required=False, help="The running mode: train or inference?")
  parser.add_argument('--ckpt_name', type=str, required=False, help="The name of the trained checkpoint. (without extension)")
  args = parser.parse_args()
  
  >>> python main.py --mode="train" --ckpt_name="best"
  ```

- `default` ï¼šé»˜è®¤å€¼
- `required` : å¿…è¦å€¼

## tokenizer

åˆ†è¯å™¨çš„æ ¸å¿ƒæ“ä½œåªæœ‰ä¸‰ä¸ªï¼štokenize, encode, decodeã€‚tokenizeè´Ÿè´£åˆ†è¯ï¼Œencodeå°†åˆ†è¯tokenè½¬æ¢æˆidï¼Œdecodeå°†idè½¬æ¢ä¸ºæ–‡æœ¬ã€‚

é¡¹ç›®é‡Œä½¿ç”¨äº†ä¸€äº›ä¹‹å‰ä¸ç†Ÿæ‚‰çš„ tokenizer æ–¹æ³•

**æ·»åŠ token** 

1. é€šè¿‡add_tokenså‡½æ•°æ·»åŠ æ–°token
2. ä½¿ç”¨resize_token_embeddingså‡½æ•°é€šçŸ¥æ¨¡å‹æ›´æ–°è¯è¡¨å¤§å°

```python
num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2']) 
model.resize_token_embeddings(len(tokenizer))
```

**æ·»åŠ ç‰¹æ®Štoken**

```python
#example1
special_tokens_dict = {'cls_token': '<CLS>'}
num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)
model.resize_token_embeddings(len(tokenizer))

#example2
special_tokens = {
    'bos_token': "<bos>",
    'additional_special_tokens': ["<sp1>", "<sp2>"],
}
num_new_tokens = tokenizer.add_special_tokens(special_tokens)
model.resize_token_embeddings(len(tokenizer))
```

ä¸æ™®é€štokenå”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œæ·»åŠ ç‰¹æ®Štokençš„å‡½æ•°add_special_tokenséœ€è¦æä¾›çš„æ˜¯å­—å…¸ï¼Œå› ä¸ºè¦æŒ‡å®šæ˜¯ä¿®æ”¹å“ªä¸€ä¸ªç‰¹æ®Šé¡¹ã€‚ç¬¬äºŒæ­¥çš„resize_token_embeddingså‡½æ•°è¿˜æ˜¯ä¸€æ ·çš„ã€‚

**transformers **  æœ‰ä»¥ä¸‹æ¥å£æä¾›

```python
SPECIAL_TOKENS_ATTRIBUTES = [
 	 "bos_token",
   "eos_token",
   "unk_token",
   "sep_token",
   "pad_token",
   "cls_token",
   "mask_token",
   "additional_special_tokens",
]
```

## pin_mamory

**pin_memory** å°±æ˜¯é”é¡µå†…å­˜ï¼Œåˆ›å»ºDataLoaderæ—¶ï¼Œè®¾ç½®pin_memory=Trueï¼Œåˆ™æ„å‘³ç€ç”Ÿæˆçš„Tensoræ•°æ®æœ€å¼€å§‹æ˜¯å±äºå†…å­˜ä¸­çš„é”é¡µå†…å­˜ï¼Œ**è¿™æ ·å°†å†…å­˜çš„Tensorè½¬åˆ°GPUçš„æ˜¾å­˜å°±ä¼šæ›´å¿«ä¸€äº›**ã€‚

ä¸»æœºä¸­çš„å†…å­˜ï¼Œæœ‰ä¸¤ç§å­˜åœ¨æ–¹å¼ï¼Œä¸€æ˜¯é”é¡µï¼ŒäºŒæ˜¯ä¸é”é¡µï¼Œé”é¡µå†…å­˜å­˜æ”¾çš„å†…å®¹åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½ä¸ä¼šä¸ä¸»æœºçš„è™šæ‹Ÿå†…å­˜è¿›è¡Œäº¤æ¢ï¼ˆæ³¨ï¼šè™šæ‹Ÿå†…å­˜å°±æ˜¯ç¡¬ç›˜ï¼‰ï¼Œè€Œä¸é”é¡µå†…å­˜åœ¨ä¸»æœºå†…å­˜ä¸è¶³æ—¶ï¼Œæ•°æ®ä¼šå­˜æ”¾åœ¨è™šæ‹Ÿå†…å­˜ä¸­

## get_polynomial_decay_schedule_with_warmup

ä½¿ç”¨schdulerçš„ä½œç”¨æ˜¯ï¼šåœ¨è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆä» 0 å¼€å§‹ï¼‰ï¼Œåœ¨ä¸€å®šæ­¥æ•°ï¼ˆæ¯”å¦‚ 1000 æ­¥ï¼‰å†…é€æ¸æé«˜åˆ°æ­£å¸¸å¤§å°ï¼ˆæ¯”å¦‚0.001ï¼‰ï¼Œé¿å…æ¨¡å‹è¿‡æ—©è¿›å…¥å±€éƒ¨æœ€ä¼˜è€Œè¿‡æ‹Ÿåˆï¼›åœ¨è®­ç»ƒåæœŸå†æ…¢æ…¢å°†å­¦ä¹ ç‡é™ä½åˆ° 0ï¼Œé¿å…åæœŸè®­ç»ƒè¿˜å‡ºç°è¾ƒå¤§çš„å‚æ•°å˜åŒ–

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_polynomial_decay_schedule_with_warmup

# ä¼˜åŒ–å™¨åˆå§‹åŒ–
optim = torch.optim.AdamW(model.parameters(), lr=config['lr'])
# è®¡ç®—æ€»çš„stepæ•°
num_batches = len(train_loader)
total_train_steps = num_batches*config['num_epochs']
warmup_steps = int(0.1* total_train_steps)
# åˆå§‹åŒ–schedule
sched = get_polynomial_decay_schedule_with_warmup(optim,
                                                  num_warmup_steps=warmup_steps,
                                                  num_training_steps=total_train_steps,
                                                  power=2
                                                 )
```

- **get_constant_schedule**: ä¿æŒå›ºå®šå­¦ä¹ ç‡ä¸å˜
- **get_constant_schedule_with_warmup**: åœ¨æ¯ä¸€ä¸ª step ä¸­çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡
  ![warmup_constant_schedule](./img/warmup_constant_schedule.png)
- **get_linear_schedule_with_warmup**: ä¸¤æ®µå¼è°ƒæ•´å­¦ä¹ ç‡
  ![warmup_constant_schedule](./img/warmup_linear_schedule.png)
- **get_cosine_schedule_with_warmup**ï¼šå’Œä¸¤æ®µå¼è°ƒæ•´ç±»ä¼¼ï¼Œåªä¸è¿‡é‡‡ç”¨çš„æ˜¯ä¸‰è§’å‡½æ•°å¼çš„æ›²çº¿è°ƒæ•´
  ![warmup_constant_schedule](./img/warmup_cosine_schedule.png)
- **get_cosine_with_hard_restarts_schedule_with_warmup**ï¼šè®­ç»ƒä¸­å°†ä¸Šé¢get_cosine_schedule_with_warmup çš„è°ƒæ•´é‡å¤ n æ¬¡
  ![warmup_constant_schedule](./img/warmup_cosine_hard_restarts_schedule.png)
- **get_polynomial_decay_schedule_with_warmup**: æŒ‰æŒ‡æ•°æ›²çº¿è¿›è¡Œä¸¤æ®µå¼è°ƒæ•´

## SummaryWriter

ç”¨ä¸tensorboardç»˜å›¾

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter()

writer.add_scalar("Loss/train", train_loss, epoch)
writer.add_scalar("PPL/train", train_ppl, epoch)
writer.add_scalar("Loss/valid", valid_loss, epoch)
writer.add_scalar("PPL/valid", valid_ppl, epoch)
writer.add_scalars("Losses", {
  'train': train_loss,
  'valid': valid_loss,
}, epoch)
writer.add_scalars("PPLs", {
  'train': train_ppl,
  'valid': valid_ppl,
}, epoch)
```





# trainer.py

`trainer`æ²¡æœ‰è®¾ä¹ˆç‰¹åˆ«çš„ï¼Œä¸»è¦æ˜¯è®­ç»ƒå’ŒéªŒè¯ç±»çš„ä»£ç ã€‚è¿™ä¸ªé¡¹ç›®ä¸å¸¸è§„`trainer`æ¯”æ²¡ä¼ å…¥`criterion`å‚æ•°æ˜¯å› ä¸º`huggingface-gpt2`æ¨¡å‹è‡ªå¸¦`loss`

è¯¦ç»†çš„å¯ä»¥å‚è€ƒ[ä¼˜é›…çš„trainer](https://zhuanlan.zhihu.com/p/414843341) çš„å‰åŠéƒ¨åˆ† æˆ–è€… ä½¿ç”¨ `Pytorch-lighting`

## tqdm

**tqdm**æ˜¯ä¸€ä¸ªå¿«é€Ÿï¼Œå¯æ‰©å±•çš„Pythonè¿›åº¦æ¡ï¼Œå¯ä»¥åœ¨ Python é•¿å¾ªç¯ä¸­æ·»åŠ ä¸€ä¸ªè¿›åº¦æç¤ºä¿¡æ¯ï¼Œç”¨æˆ·åªéœ€è¦å°è£…ä»»æ„çš„è¿­ä»£å™¨ tqdm(iterator)ã€‚

```python
for i, batch in enumerate(tqdm(self.train_loader)):
```

## æ¨¡å‹è®­ç»ƒè€ä¸‰æ · ğŸ¤–ï¸

æ¨¡å‹ä¸€èˆ¬è®­ç»ƒä¸€å®šè¦æœ‰ä¸‰ä¸ªæ­¥éª¤

0. æ¸…ç©ºæ¢¯åº¦

1. è®¡ç®—loss

2. åå‘ä¼ æ’­backward

3. step

   ```python
   loss, logits= outputs[0], outputs[1]
   self.optim.zero_grad()
   loss.backward()
   self.optim.step()
   self.sched.step()
   ```

## å›°æƒ‘åº¦

å›°æƒ‘åº¦ ppl  = exp {loss} 

## .train()å’Œ.eval()

model.train( ) ï¼šå¯ç”¨ BatchNormalization å’Œ Dropout

model.eval( ) ï¼šä¸å¯ç”¨ BatchNormalization å’Œ Dropout

```python
# åœ¨è®­ç»ƒæ—¶æˆ‘ä»¬ä½¿ç”¨
self.model.train()
for i, batch in enumerate(tqdm(self.train_loader)):

# åœ¨éªŒè¯å’Œæœ€ç»ˆäº¤äº’æ—¶ä½¿ç”¨
self.model.eval()	
with torch.no_grad():
    for i, batch in enumerate(tqdm(self.valid_loader)):
```





# predictor.py

## inferä»£ç æ³¨é‡Š

```python
def infer(self):
  	print("Let's start!")
  	print(f"If you want to quit the conversation, please type \"{config['end_command']}\".")

  	self.model.eval()
  	seed_everything(config['seed'])

  	with torch.no_grad():
        input_hists = []
        # ç”¨æˆ·æ˜¯speaker1 æ¨¡å‹æ˜¯speaker2
        while True:
            utter = input("You: ")
            # å¦‚æœæ˜¯â€œAbort!â€ é‚£ä¹ˆå°±ç»“æŸ
            if utter == config['end_command']:
                print("Bot: Good bye.")
                break
            # å°†ç”¨æˆ·è¯´çš„è¯å‰é¢åŠ ä¸Šsp1_id ä¹‹åæ”¾å…¥å¯¹è¯å†å²input_hists(idsæ ¼å¼)
            input_ids = [self.sp1_id] + self.tokenizer.encode(utter)
            input_hists.append(input_ids)

            # å¦‚è¿‡å¯¹è¯å†å²input_histsè¶…è¿‡ æœ€å¤§è½®æ¬¡config['max_turns']äº†ï¼Œä»å‰é¢æˆªæ–­
            if len(input_hists) >= config['max_turns']:
                num_exceeded = len(input_hists) - config['max_turns']+1
                input_hists = input_hists[num_exceeded:]

            # æŠŠinput_histsé“ºå¹³ï¼Œå‰é¢åŠ ä¸Šbos_idï¼Œåé¢åœ¨åŠ ä¸Šsp2_id(å› ä¸ºæ¯ä¸€æ¬¡å¾ªç¯ç»è¿‡åˆ°è¿™è¾¹çš„æ—¶å€™æœ€åä¸€å¥è¯ä¸€å®šæ˜¯sp1è¯´çš„)
            input_ids = [self.bos_id] + list(chain.from_iterable(input_hists)) + [self.sp2_id]
            # input_histsæœ€å¼€å§‹çš„è¯æ˜¯è°è¯´çš„ ->ä¸ºäº†ä¸‹é¢ç”Ÿæˆtoken_type_ids
            start_sp_id = input_hists[0][0]
            # input_histsä¹‹åçš„è¯æ˜¯è°è¯´çš„ ->ä¸ºäº†ä¸‹é¢ç”Ÿæˆtoken_type_ids
            next_sp_id = self.sp1_id if start_sp_id == self.sp2_id else self.sp2_id
            token_type_ids = [[start_sp_id] * len(hist) if h % 2 == 0 else [next_sp_id] * len(hist) for h, hist in enumerate(input_hists)]
            assert len(token_type_ids) == len(input_hists)
            # è¿™é‡Œä¸ä¸Šé¢è¿™å‡ è¯å¯¹åº”èµ·æ¥äº† bosä¹Ÿå±äºstart_sp_id
            # input_ids = [self.bos_id] + list(chain.from_iterable(input_hists)) + [self.sp2_id]
            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [self.sp2_id]
            assert len(input_ids) == len(token_type_ids)
            input_len = len(input_ids)

            #unsqueeze:æ‰©å……æ•°æ®ç»´åº¦ï¼Œåœ¨0èµ·çš„æŒ‡å®šä½ç½®NåŠ ä¸Šç»´æ•°ä¸º1çš„ç»´åº¦
            #squeeze: ç»´åº¦å‹ç¼©ï¼Œåœ¨0èµ·çš„æŒ‡å®šä½ç½®Nï¼Œå»æ‰ç»´æ•°ä¸º1çš„çš„ç»´åº¦
            # è¿™é‡Œå¢åŠ batch_sizeè¿™ä¸€ç»´åº¦
            input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(config['device'])
            token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(config['device'])
            output_ids = self.nucleus_sampling(input_ids, token_type_ids, input_len)
            res = self.tokenizer.decode(output_ids, skip_special_tokens=True)
            print(f"Bot: {res}")
            input_hists.append([self.sp2_id] + self.tokenizer.encode(res))
```

## nucleus_samplingä»£ç æ³¨é‡Š

**Beam Search**

åœ¨å½“å‰çº§åˆ«çš„çŠ¶æ€ä¸‹è®¡ç®—æ‰€æœ‰å¯èƒ½æ€§ï¼Œå¹¶æŒ‰ç…§é€’å¢é¡ºåºå¯¹ä»–ä»¬è¿›è¡Œæ’åºï¼Œä½†åªä¿ç•™ä¸€å®šæ•°é‡çš„å¯èƒ½ç»“æœï¼ˆä¾æ®**Beam Width**å†³å®šæ•°é‡ï¼‰ï¼Œæ¥ç€æ ¹æ®è¿™äº›å¯èƒ½ç»“æœè¿›è¡Œæ‰©å±•ï¼Œè¿­ä»£ä»¥ä¸Šçš„åŠ¨ä½œç›´åˆ°æœç´¢ç»“æŸå¹¶è¿”å›æœ€ä½³è§£ï¼ˆå…·æœ‰æœ€é«˜æ¦‚ç‡çš„é‚£ä¸ªï¼‰ã€‚
å‡è®¾è¯è¡¨å¤§å°ä¸º3ï¼ŒåŒ…å«[A, B, C]ï¼ŒBeam Widthä¸º2

1. ç”Ÿæˆç¬¬1ä¸ªè¯çš„æ—¶å€™ï¼Œå¯¹P(A)ã€P(B)ã€P(C)è¿›è¡Œæ’åºï¼Œé€‰å–æ¦‚ç‡æœ€å¤§çš„ä¸¤ä¸ªï¼Œå‡è®¾ä¸ºAï¼ŒC
2. ç”Ÿæˆç¬¬2ä¸ªè¯çš„æ—¶å€™ï¼Œå°†å½“å‰åºåˆ—Aï¼ŒCåˆ†åˆ«å’Œè¯è¡¨ä¸­çš„æ‰€æœ‰è¯è¿›è¡Œç»„åˆï¼Œå¾—åˆ°æ–°çš„6ä¸ªåºåˆ—ä¸ºAAã€ABã€ACï¼ŒCAã€CBã€CCï¼Œç„¶ååŒæ ·å–æ¦‚ç‡æœ€å¤§çš„ä¸¤ä¸ªä½œä¸ºå½“å‰åºåˆ—ï¼Œå‡è®¾ä¸ºAAã€CC
3. é‡å¤ä»¥ä¸Šçš„è¿‡ç¨‹ï¼Œç›´åˆ°é‡åˆ°ç»“æŸç¬¦ä¸ºæ­¢ï¼Œæœ€ç»ˆè¾“å‡º2ä¸ªå¾—åˆ†æœ€é«˜çš„åºåˆ—

**Top-k Sampling**

åœ¨è§£ç çš„æ¯ä¸ªæ—¶é—´æ­¥ä»å‰kä¸ªæ¦‚ç‡æœ€å¤§çš„è¯ä¸­æŒ‰å®ƒä»¬çš„æ¦‚ç‡è¿›è¡Œé‡‡æ ·ã€‚

é—®é¢˜ï¼štop-k samplingä¸­kçš„é€‰æ‹©æ˜¯ä¸ªéš¾é¢˜ï¼Œé€‰å¤§äº†å¯èƒ½ä¼šé‡‡æ ·å‡ºé•¿å°¾è¯ï¼Œå¯¼è‡´è¯­å¥ä¸é€šé¡ºï¼Œé€‰å°äº†åˆé€€åŒ–æˆäº†Beam Search

**numcleus_sampleing**  

æ˜¯ beam_search çš„å‡çº§ç‰ˆï¼ï¼Beam Searchä¼šæ€»ä¼šé€‰æ‹©æœ€ç¬¦åˆè¯­è¨€æ¨¡å‹çš„è¯æ±‡ï¼Œå› æ­¤ç”Ÿæˆçš„æ–‡æœ¬æ²¡æœ‰æ–°æ„(less surprising)ã€‚ä¹‹åæå‡ºäº†ä¸€ç§top-k samplingçš„æ”¹è¿›æ–¹æ¡ˆæ¥è§£å†³é—®é¢˜ï¼šnucleus sampling (top-p sampling)

ç»™å®šä¸€ä¸ªæ¦‚ç‡é˜ˆå€¼pï¼Œä»è§£ç è¯å€™é€‰é›†ä¸­é€‰æ‹©ä¸€ä¸ªæœ€å°é›†Vpï¼Œä½¿å¾—å®ƒä»¬å‡ºç°çš„æ¦‚ç‡å’Œå¤§äºç­‰äºpã€‚ç„¶åå†å¯¹Vpåšä¸€æ¬¡re-scalingï¼Œæœ¬æ—¶é—´æ­¥ä»…ä»Vpé›†åˆä¸­è§£ç 

```python
def nucleus_sampling(self, input_ids, token_type_ids, input_len):
    output_ids = []
    for pos in range(input_len, config['max_len']):
        # æ²¡æä¾›label,logitsä¸ºç¬¬ä¸€ä¸ª,å½¢çŠ¶[batch_size,seq_len,V]
      	# tensor([[ 2.,  5.,  3.,  4.,  1., 10.]])
        output = self.model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos - 1]  # (1, V)
        #tensor([[3.3195e-04, 6.6674e-03, 9.0233e-04, 2.4528e-03, 1.2212e-04, 9.8952e-01]])
        output = F.softmax(output, dim=-1)  # (1, V)
				#é™åºæ’åº
        # tensor([[9.8952e-01, 6.6674e-03, 2.4528e-03, 9.0233e-04, 3.3195e-04, 1.2212e-04]])
        # tensor([[5, 1, 3, 2, 0, 4]])
        sorted_probs, sorted_idxs = torch.sort(output, descending=True)
        # tensor([[0.9895, 0.9962, 0.9986, 0.9995, 0.9999, 1.0000]])
        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)
        # å‡è®¾top_pæ˜¯0.9995
        # tensor([[False, False, False,  True,  True,  True]])
        idx_remove = cumsum_probs > config['top_p']
        idx_remove[:, 1:] = idx_remove[:, :-1].clone()
        # tensor([[False, False, False, False,  True,  True]])
        idx_remove[:, 0] = False
        # tensor([[9.8952e-01, 6.6674e-03, 2.4528e-03, 9.0233e-04, 0.0000e+00, 0.0000e+00]])
        sorted_probs[idx_remove] = 0.0
        # tensor([[9.8997e-01, 6.6704e-03, 2.4539e-03, 9.0274e-04, 0.0000e+00, 0.0000e+00]])
        sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)
				# æ”¾å›åŸæ¥çš„ä½ç½®
        # tensor([[0.0000e+00, 6.6704e-03, 9.0274e-04, 2.4539e-03, 0.0000e+00, 9.8997e-01]])
        probs = torch.zeros(output.shape, device=config['device']).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)
        # é‡‡æ ·ä¸€ä¸ª tensor([[5]])
        idx = torch.multinomial(probs, 1)  # (1, 1)
				# 5
        idx_item = idx.squeeze(-1).squeeze(-1).item()
        output_ids.append(idx_item)

        if idx_item == self.eos_id:
            break
				# è‡ªå›å½’
        input_ids = torch.cat((input_ids, idx), dim=-1)
        next_type_id = torch.LongTensor([[self.sp2_id]]).to(config['device'])
        token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)
        assert input_ids.shape == token_type_ids.shape

    return output_ids
```

å‡ ä¸ªä¹‹å‰é‡åˆ°æ¯”è¾ƒå°‘çš„å‡½æ•°

1. **torch.multinomial(input, num_samples, replacement=False, out=None)**

   ä½œç”¨æ˜¯å¯¹inputçš„æ¯ä¸€è¡Œåšn_samplesæ¬¡å–å€¼ï¼Œè¾“å‡ºçš„å¼ é‡æ˜¯æ¯ä¸€æ¬¡å–å€¼æ—¶inputå¼ é‡å¯¹åº”è¡Œçš„**ä¸‹æ ‡**

   `input (Tensor)` â€“ the input tensor containing probabilities
   `num_samples (int)` â€“ number of samples to draw
   `replacement (bool, optional)`  â€“ whether to draw with replacement or not
   `out (Tensor, optional) ` â€“ the output tensor

2. **scatter_(input, dim, index, src)**
   å°†srcä¸­æ•°æ®æ ¹æ®indexä¸­çš„ç´¢å¼•æŒ‰ç…§dimçš„æ–¹å‘å¡«è¿›inputä¸­

   

# process_data.py

åˆ†åˆ«å®šä¹‰åŠ è½½å››ä¸ªæ•°æ®é›†çš„æ–¹æ³•

å››ä¸ªæ•°æ®é›†ï¼šblended_skill_talkï¼Œpersona_chatï¼Œempatheticï¼Œdaily

|     æ•°æ®é›†åç§°     | è®­ç»ƒé›†é•¿åº¦ | éªŒè¯é›†é•¿åº¦ | æµ‹è¯•é›†é•¿åº¦ |
| :----------------: | :--------: | :--------: | :--------: |
|       Daily        |   11118    |    1000    |    1000    |
|     empathetic     |   76673    |   12030    |   10943    |
|    persona_chat    |   17878    |    1000    |     -      |
| blended_skill_talk |    4819    |    4819    |    980     |

**GPT2 tokenizer** : gpt2 tokenizer å’Œ bert tokenizerä¸åŒï¼Œæœ‰ä¸€äº›ç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒ [å…³äºGPT2Tokenizerçš„ä¸€äº›å‘ç°](https://blog.csdn.net/qq_34418352/article/details/106627193)

**process_token_list** : [é›¨å“¥ä¸ºè¿™ä¸ªé¡¹ç›®å†™çš„æ³¨é‡Š](https://github.com/qitianyuu/nlp_ChatRobot/blob/main/src/process_data.py)

**å››ä¸ªæ•°æ®é›†å„å¼ä¸åŒï¼Œå¤„ç†èµ·æ¥è¦è€å¿ƒä»”ç»†**

# mydataset.py

## CustomDatasetä»£ç æ³¨é‡Š

**torch.nn.CrossEntropyLoss(weight=None, size_average=None,ignore_index=-100, reduce=None, reduction=â€˜meanâ€™)**

`weight`ï¼šä¸å¿…å¤šè¯´ï¼Œè¿™å°±æ˜¯å„classçš„æƒé‡ã€‚
`reduction`ï¼šç»“æœçš„è§„çº¦æ–¹å¼ï¼Œå–å€¼ç©ºé—´ä¸º{'mean', 'none', 'sum}ã€‚ç”±äºä½ ä¼ å…¥ nn.CrossEntropyLoss()çš„è¾“å…¥æ˜¯ä¸€ä¸ªbatchï¼Œé‚£ä¹ˆæŒ‰ç†è¯´å¾—åˆ°çš„äº¤å‰ç†µæŸå¤±åº”è¯¥æ˜¯ batchä¸ªlossã€‚å½“å‰é»˜è®¤çš„å¤„ç†æ–¹å¼æ˜¯ï¼Œå¯¹ batch ä¸ªæŸå¤±å–å¹³å‡ï¼›ä¹Ÿå¯ä»¥é€‰æ‹©ä¸åšè§„çº¦ï¼›æˆ–è€…å°†batchä¸ªæŸå¤±å–åŠ å’Œ;
`ignore_index` ï¼šåšäº¤å‰ç†µè®¡ç®—æ—¶ï¼Œè‹¥è¾“å…¥ä¸ºignore_indexæŒ‡å®šçš„æ•°å€¼ï¼Œåˆ™è¯¥æ•°å€¼ä¼šè¢«å¿½ç•¥ï¼Œä¸å‚ä¸äº¤å‰ç†µè®¡ç®—

```python
class CustomDataset(Dataset):
    def __init__(self, prefix, config):
        # å¥å£®æ€§æ£€æŸ¥
        assert prefix == config['train_prefix'] or prefix == config['valid_prefix']

        print(f"Loading {prefix}_id.json.")
        with open(f"{config['data_dir']}/{prefix}_ids.json", 'r') as f:
            dials = json.load(f)

        self.input_ids = []        
        self.token_type_ids = []   
        self.labels = []           
				
        for dial in tqdm(dials):
            hists = []
            # å¯¹æ¯ä¸€å¥è¯å¾ªç¯ï¼ŒåŠ ä¸Šå¯¹è¯è€…IDï¼ŒåŠ å…¥hists
            for u, utter in enumerate(dial):
                if u % 2 == 0:
                    hists.append([config['sp1_id']] + utter)
                else:
                    hists.append([config['sp2_id']] + utter)
						# å¯¹äºæ¯å¥è¯å¾ªç¯
            for h in range(len(hists)):
              	# å¦‚æœæ˜¯ <speaker2>ï¼Œå›ºå®šä½hï¼Œä»0å¼€å§‹å¾€hæ‰¾
                if hists[h][0] == config['sp2_id']:
                    start = max(0, h - config['max_turns'] + 1)
                    for s in range(start, h):
                        # å¯¹è¯
                        contexts = hists[s:h + 1]
                        # é€‰å‡ºå¯¹è¯è½®æ•°åˆé€‚çš„æ•°æ® å¤§äº2å¥ï¼Œå°äº5å¥ï¼Œå› ä¸ºä¸Šé¢å¾ªç¯çš„è®¾ç½®ï¼Œä¼šä¼˜å…ˆ5å¥å­
                        #if len(contexts) > config['max_turns']:
                        #    num_exceeded = len(contexts) - config['max_turns']
                        #    contexts = contexts[num_exceeded:]
                        if len(contexts) < 2:
                            break
                        # åŠ ä¸Šç‰¹æ®Šå ä½ç¬¦boså’Œeos
                        input_ids = [config['bos_id']] + list(chain.from_iterable(contexts)) + [config['eos_id']]

                        if len(input_ids) <= config['max_len']:
                            # ä¸º input_ids çš„æ¯ä¸ªä½ç½®æ ‡æ³¨ä¸Šæ˜¯è°è¯´çš„è¯, åŒæ—¶åŠ ä¸Šå¼€å§‹æ ‡å¿—å’Œä¸‹ä¸ªspeakerçš„å¼€å§‹æ ‡å¿—
                            start_sp_id, next_sp_id = contexts[0][0], contexts[1][0]
                            # ç”Ÿæˆtoken_type_ids
                            token_type_ids = [[start_sp_id] * len(ctx) if c % 2 == 0 else [next_sp_id] * len(ctx) for c, ctx in enumerate(contexts)] 
                            # åˆ¤æ–­ä¸€ä¸‹ æœ€æœ‰ä¸€å¥è¯åº”è¯¥æ˜¯spkear2ï¼ˆä¹Ÿå°±æ˜¯æ¨¡å‹ï¼‰è¯´çš„
                            assert token_type_ids[-1][0] == config['sp2_id']
                            # start_sp_idç»™input_idsé‡Œçš„bos_id  config['sp2_id']ç»™inputé‡Œçš„eos_id  
                            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [config['sp2_id']]
                            # æ£€æŸ¥ä¸¤è€…é•¿åº¦ç›¸ç­‰
                            assert len(input_ids) == len(token_type_ids)
                            # é™¤äº†æœ€åä¸€å¥è¯ï¼Œå…¶ä»–å…¨éƒ¨æ ‡æ³¨ä¸º -100ï¼ˆæœ€åä¸€å¥è¯çš„sp2_idä¹Ÿæ˜¯-100ï¼‰
                            labels = [[-100] * len(ctx) if c < len(contexts) - 1 else [-100] + ctx[1:] for c, ctx in enumerate(contexts)]
                            assert labels[-1][1:] == contexts[-1][1:]
                            # bos -100  eosè¦è®¡ç®—æŸå¤±
                            labels = [-100] + list(chain.from_iterable(labels)) + [config['eos_id']]
                            assert len(input_ids) == len(labels)
                            self.input_ids.append(input_ids)
                            self.token_type_ids.append(token_type_ids)
                            self.labels.append(labels)
                             # å½“æ‰¾åˆ°ä¸€ä¸ªæ»¡è¶³çš„å¯¹è¯ï¼Œå³è·³å‡ºå¾ªç¯ï¼Œå¯»æ‰¾ä¸‹ä¸€ä¸ªæ»¡è¶³çš„å¯¹è¯
                            break

    def __getitem__(self, idx):
        return self.input_ids[idx], self.token_type_ids[idx], self.labels[idx]

    def __len__(self):
        return len(self.input_ids)
```

## PadCollateæ³¨é‡Š

**torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0)**

è¿™æ˜¯padæ“ä½œï¼Œsequencesä¹Ÿæ˜¯listã€‚`torch.nn.utils.rnn.pad_sequence`ç»™listé‡Œçš„tensoréƒ½ç”¨padding_valueæ¥padæˆæœ€é•¿çš„é•¿åº¦ï¼Œå¹¶ç»„åˆæˆä¸€ä¸ªtensorã€‚

- **sequences** (*list*) â€“ list of variable length sequences.
- **batch_first** (*bool*, *optional*) â€“ output will be in `B x T x *` if True, or in `T x B x *` otherwise.  where T is the length of the longest
- **padding_value** (*float*,*optional*) â€“ value for padded elements. Default: 0.

```python
class PadCollate():
    def __init__(self, eos_id):
        self.eos_id = eos_id

    def pad_collate(self, batch):
        input_ids, token_type_ids, labels = [], [], []
        for idx, seqs in enumerate(batch):
            input_ids.append(torch.LongTensor(seqs[0]))
            token_type_ids.append(torch.LongTensor(seqs[0]))
            labels.append(torch.LongTensor(seqs[2]))

        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.eos_id)
        token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids, batch_first=True, padding_value=self.eos_id)
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)

        return input_ids, token_type_ids, labels
```



# load_data.py

è°ƒç”¨process_data.py å°†å››ä¸ªä¸åŒçš„æ•°æ®é›†åˆå¹¶ä¿å­˜ä¸ºjsonå½¢å¼

# utils.py

å·¥å…·ç±»ï¼Œæœ¬é¡¹ç›®å·¥å…·å‡½æ•°åªæœ‰ä¸€ä¸ª **seed_everything**

# settings.py

æœ¬é¡¹ç›®æ‰€æœ‰çš„å‚æ•°é…ç½®ä¿¡æ¯

```python
config = {
    'ckpt_dir': 'saved_models',
    'seed': 42,
    'device': 'cuda' if cuda.is_available() else 'cpu',
    'train_frac': 0.85,
    'data_dir': 'data',
    'model_name': 'gpt2',
    'train_prefix': 'train',
    'valid_prefix': 'valid',
    'max_turns': 5,
    'max_len': 1024,
    'lr': 2e-5,
    'batch_size': 8,
    'num_workers': 0,
    'num_epochs': 5,
    'warmup_ratio': 0.1,
    'bos_token': '<bos>',
    'sp1_token': '<sp1>',
    'sp2_token': '<sp2>',
    'end_command': 'Abort!',
    'top_p':0.8
}
```











# å­¦ä¹ èµ„æº

1. [devjwsong/gpt2-dialogue-generation-pytorch](https://github.com/devjwsong/gpt2-dialogue-generation-pytorch)

2. [ğŸ¦„ How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)

3. [huggingface/transfer-learning-conv-ai](https://github.com/huggingface/transfer-learning-conv-ai)



